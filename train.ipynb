{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79fdd771-aada-4898-901e-b733c833134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from common.arguments import parse_args\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "\n",
    "from common.camera import *\n",
    "from common.model import *\n",
    "from common.loss import *\n",
    "from common.generators import ChunkedGenerator, UnchunkedGenerator\n",
    "from time import time\n",
    "from common.utils import deterministic_random\n",
    "\n",
    "from common.h36m_dataset import Human36mDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f05fe11b-e8a3-44e4-ae1f-fd7041ff2478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h36m\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    dataset = 'h36m'\n",
    "    keypoints = 'gt'\n",
    "    subjects_train = ['S1','S5','S6','S7','S8']\n",
    "    subjects_test = ['S9','S11']\n",
    "    subjects_unlabeled = ''\n",
    "    actions = '*'\n",
    "    checkpoint = None\n",
    "    render = False\n",
    "    data_augmentation = True\n",
    "    \n",
    "    #Model\n",
    "    stride = 1\n",
    "    epochs = 60\n",
    "    batch_size = 1024\n",
    "    dropout = 0.25\n",
    "    learning_rate = 0.001\n",
    "    lr_decay = 0.95\n",
    "    architecture = '3,3,3'\n",
    "    causal = True\n",
    "    channels = 1024\n",
    "    \n",
    "    #Experimental\n",
    "    subset = 1\n",
    "    downsample = 1\n",
    "    warmup = 1\n",
    "    disable_optimizations = False\n",
    "    dense = False\n",
    "    \n",
    "args = Args()\n",
    "print(args.dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255a56b8-d610-43f7-bc7d-c6930146c592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_3d_h36m.npz\n"
     ]
    }
   ],
   "source": [
    "dataset_path = dataset_path = 'data/data_3d_' + args.dataset + '.npz'\n",
    "print(dataset_path)\n",
    "dataset = Human36mDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26fd6c69-dfc4-449a-8108-90f9e333f615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 3d data...\n"
     ]
    }
   ],
   "source": [
    "print('Preparing 3d data...')\n",
    "for subject in dataset.subjects():\n",
    "    for action in dataset[subject].keys():\n",
    "        anim = dataset[subject][action]\n",
    "        \n",
    "        if 'positions' in anim:\n",
    "            positions_3d = []\n",
    "            for cam in anim['cameras']:\n",
    "                pos_3d = world_to_camera(anim['positions'], R=cam['orientation'], t=cam['translation'])\n",
    "                pos_3d[:, 1:] -= pos_3d[:, :1] # Remove global offset, but keep trajectory in first position\n",
    "                positions_3d.append(pos_3d)\n",
    "            anim['positions_3d'] = positions_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748a384f-4621-4de3-8e4a-7265c100efeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D detections...\n"
     ]
    }
   ],
   "source": [
    "print('Loading 2D detections...')\n",
    "keypoints = np.load('data/data_2d_' + args.dataset + '_' + args.keypoints + '.npz', allow_pickle=True)\n",
    "keypoints_metadata = keypoints['metadata'].item()\n",
    "keypoints_symmetry = keypoints_metadata['keypoints_symmetry']\n",
    "kps_left, kps_right = list(keypoints_symmetry[0]), list(keypoints_symmetry[1])\n",
    "joints_left, joints_right = list(dataset.skeleton().joints_left()), list(dataset.skeleton().joints_right())\n",
    "keypoints = keypoints['positions_2d'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1769dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in dataset.subjects():\n",
    "    assert subject in keypoints, 'Subject {} is missing from the 2D detections dataset'.format(subject)\n",
    "    for action in dataset[subject].keys():\n",
    "        assert action in keypoints[subject], 'Action {} of subject {} is missing from the 2D detections dataset'.format(action, subject)\n",
    "        if 'positions_3d' not in dataset[subject][action]:\n",
    "            continue\n",
    "            \n",
    "        for cam_idx in range(len(keypoints[subject][action])):\n",
    "            \n",
    "            # We check for >= instead of == because some videos in H3.6M contain extra frames\n",
    "            mocap_length = dataset[subject][action]['positions_3d'][cam_idx].shape[0]\n",
    "            assert keypoints[subject][action][cam_idx].shape[0] >= mocap_length\n",
    "            \n",
    "            if keypoints[subject][action][cam_idx].shape[0] > mocap_length:\n",
    "                # Shorten sequence\n",
    "                keypoints[subject][action][cam_idx] = keypoints[subject][action][cam_idx][:mocap_length]\n",
    "\n",
    "        assert len(keypoints[subject][action]) == len(dataset[subject][action]['positions_3d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f538fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in keypoints.keys():\n",
    "    for action in keypoints[subject]:\n",
    "        for cam_idx, kps in enumerate(keypoints[subject][action]):\n",
    "            # Normalize camera frame\n",
    "            cam = dataset.cameras()[subject][cam_idx]\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=cam['res_w'], h=cam['res_h'])\n",
    "            keypoints[subject][action][cam_idx] = kps\n",
    "subjects_train = args.subjects_train\n",
    "subjects_semi = [] if not args.subjects_unlabeled else args.subjects_unlabeled.split(',')\n",
    "if not args.render:\n",
    "    subjects_test = args.subjects_test\n",
    "else:\n",
    "    subjects_test = [args.viz_subject]\n",
    "\n",
    "semi_supervised = len(subjects_semi) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3ed0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(subjects, action_filter=None, subset=1, parse_3d_poses=True):\n",
    "    out_poses_3d = []\n",
    "    out_poses_2d = []\n",
    "    out_camera_params = []\n",
    "    for subject in subjects:\n",
    "        for action in keypoints[subject].keys():\n",
    "            ############################\n",
    "            if action_filter is not None:\n",
    "                found = False\n",
    "                for a in action_filter:\n",
    "                    if action.startswith(a):\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    continue\n",
    "            ############################    \n",
    "            poses_2d = keypoints[subject][action]\n",
    "            for i in range(len(poses_2d)): # Iterate across cameras\n",
    "                out_poses_2d.append(poses_2d[i])\n",
    "            # 카메라 4방향 이어주기\n",
    "            \n",
    "            if subject in dataset.cameras():\n",
    "                cams = dataset.cameras()[subject]\n",
    "                assert len(cams) == len(poses_2d), 'Camera count mismatch'\n",
    "                for cam in cams:\n",
    "                    if 'intrinsic' in cam:\n",
    "                        out_camera_params.append(cam['intrinsic'])\n",
    "            # 카메라 내부 파라미터\n",
    "            \n",
    "            if parse_3d_poses and 'positions_3d' in dataset[subject][action]:\n",
    "                poses_3d = dataset[subject][action]['positions_3d']\n",
    "                assert len(poses_3d) == len(poses_2d), 'Camera count mismatch'\n",
    "                for i in range(len(poses_3d)): # Iterate across cameras\n",
    "                    out_poses_3d.append(poses_3d[i])\n",
    "    \n",
    "    if len(out_camera_params) == 0:\n",
    "        out_camera_params = None\n",
    "    if len(out_poses_3d) == 0:\n",
    "        out_poses_3d = None\n",
    "    \n",
    "    stride = args.downsample\n",
    "    if subset < 1:\n",
    "        for i in range(len(out_poses_2d)):\n",
    "            n_frames = int(round(len(out_poses_2d[i])//stride * subset)*stride)\n",
    "            start = deterministic_random(0, len(out_poses_2d[i]) - n_frames + 1, str(len(out_poses_2d[i])))\n",
    "            out_poses_2d[i] = out_poses_2d[i][start:start+n_frames:stride]\n",
    "            if out_poses_3d is not None:\n",
    "                out_poses_3d[i] = out_poses_3d[i][start:start+n_frames:stride]\n",
    "    elif stride > 1:\n",
    "        # Downsample as requested\n",
    "        for i in range(len(out_poses_2d)):\n",
    "            out_poses_2d[i] = out_poses_2d[i][::stride]\n",
    "            if out_poses_3d is not None:\n",
    "                out_poses_3d[i] = out_poses_3d[i][::stride]\n",
    "    \n",
    "\n",
    "    return out_camera_params, out_poses_3d, out_poses_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d089b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_filter = None if args.actions == '*' else args.actions.split(',')\n",
    "if action_filter is not None:\n",
    "    print('Selected actions:', action_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8816db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras_valid, poses_valid, poses_valid_2d = fetch(subjects_test, action_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79514831",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_widths = [int(x) for x in args.architecture.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77593f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 생성\n",
    "if not args.disable_optimizations and not args.dense and args.stride == 1:\n",
    "    # Use optimized model for single-frame predictions\n",
    "    model_pos_train = TemporalModelOptimized1f(poses_valid_2d[0].shape[-2], poses_valid_2d[0].shape[-1], dataset.skeleton().num_joints(),\n",
    "                                filter_widths=filter_widths, causal=args.causal, dropout=args.dropout, channels=args.channels)\n",
    "else:\n",
    "    # When incompatible settings are detected (stride > 1, dense filters, or disabled optimization) fall back to normal model\n",
    "    model_pos_train = TemporalModel(poses_valid_2d[0].shape[-2], poses_valid_2d[0].shape[-1], dataset.skeleton().num_joints(),\n",
    "                                filter_widths=filter_widths, causal=args.causal, dropout=args.dropout, channels=args.channels,\n",
    "                                dense=args.dense)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83fa84b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Receptive field: 27 frames\n"
     ]
    }
   ],
   "source": [
    "model_pos = TemporalModel(poses_valid_2d[0].shape[-2], poses_valid_2d[0].shape[-1], dataset.skeleton().num_joints(),\n",
    "                            filter_widths=filter_widths, causal=args.causal, dropout=args.dropout, channels=args.channels,\n",
    "                            dense=args.dense)\n",
    "\n",
    "receptive_field = model_pos.receptive_field()\n",
    "print('INFO: Receptive field: {} frames'.format(receptive_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d985064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using causal convolutions\n"
     ]
    }
   ],
   "source": [
    "pad = (receptive_field - 1) // 2 # Padding on each side\n",
    "if args.causal:\n",
    "    print('INFO: Using causal convolutions')\n",
    "    causal_shift = pad\n",
    "else:\n",
    "    causal_shift = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3eb7a778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Trainable parameter count: 8555571\n"
     ]
    }
   ],
   "source": [
    "model_params = 0\n",
    "for parameter in model_pos.parameters():\n",
    "    model_params += parameter.numel()\n",
    "print('INFO: Trainable parameter count:', model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e90b8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model_pos = model_pos.cuda()\n",
    "    model_pos_train = model_pos_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae938e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Training on 1559752 frames\n"
     ]
    }
   ],
   "source": [
    "#Train 일 경우\n",
    "\n",
    "cameras_train, poses_train, poses_train_2d = fetch(subjects_train, action_filter, subset=args.subset)\n",
    "\n",
    "lr = args.learning_rate\n",
    "\n",
    "optimizer = optim.Adam(model_pos_train.parameters(), lr=lr, amsgrad=True)\n",
    "\n",
    "lr_decay = args.lr_decay\n",
    "\n",
    "losses_3d_train = []\n",
    "losses_3d_train_eval = []\n",
    "losses_3d_valid = []\n",
    "\n",
    "epoch = 0\n",
    "initial_momentum = 0.1\n",
    "final_momentum = 0.001\n",
    "\n",
    "\n",
    "train_generator = ChunkedGenerator(args.batch_size//args.stride, cameras_train, poses_train, poses_train_2d, args.stride,\n",
    "                                   pad=pad, causal_shift=causal_shift, shuffle=True, augment=args.data_augmentation,\n",
    "                                   kps_left=kps_left, kps_right=kps_right, joints_left=joints_left, joints_right=joints_right)\n",
    "train_generator_eval = UnchunkedGenerator(cameras_train, poses_train, poses_train_2d,\n",
    "                                          pad=pad, causal_shift=causal_shift, augment=False)\n",
    "print('INFO: Training on {} frames'.format(train_generator_eval.num_frames()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51fb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea690d848a8e4f0190fd719c20f1157f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ca5c23e50643e983f56b9e9ca67d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17ca16acb744ace80df67cc63ce5171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a362e68807824bdfa7619becc98f964e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344f86fbdbf544c1af99553cd42833b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4709be0842604beea6702cd301c61c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb637f48029b4f30b4d65ae0842ab86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "while epoch < args.epochs:\n",
    "        start_time = time()\n",
    "        epoch_loss_3d_train = 0\n",
    "        N = 0\n",
    "        N_semi = 0\n",
    "        model_pos_train.train()\n",
    "        \n",
    "        for _, batch_3d, batch_2d in tqdm(train_generator.next_epoch()):\n",
    "                inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "                inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs_3d = inputs_3d.cuda()\n",
    "                    inputs_2d = inputs_2d.cuda()\n",
    "                inputs_3d[:, :, 0] = 0\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Predict 3D poses\n",
    "                predicted_3d_pos = model_pos_train(inputs_2d)\n",
    "                loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "                epoch_loss_3d_train += inputs_3d.shape[0]*inputs_3d.shape[1] * loss_3d_pos.item()\n",
    "                N += inputs_3d.shape[0]*inputs_3d.shape[1]\n",
    "\n",
    "                loss_total = loss_3d_pos\n",
    "                loss_total.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "        losses_3d_train.append(epoch_loss_3d_train / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73684517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
